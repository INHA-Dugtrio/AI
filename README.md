# AI
인하대학교 24-2 종합설계 3조 닥트리오 프로젝트의 이상치 탐지 모델에 대한 구현 코드입니다. 
LSTM-AutoEncoder를 이용해 여러 스트리밍 데이터에 대한 이상치를 탐지하여 서버사이드로 알림 발송 여부를 전하게 됩니다. 

---

## **1. LSTM-AutoEncoder란?**

### **1.1 AutoEncoder의 기본 원리**

- **AutoEncoder**는 입력 데이터를 압축한 후 다시 복원하는 신경망 구조입니다.
- 일반적으로 **입력과 출력이 동일**하며, 중간에 **병목 지점(bottleneck layer)**을 만들어 데이터의 주요 특징을 추출합니다.
- **학습 목표**는 입력 데이터를 최대한 정확하게 복원하도록 네트워크의 가중치를 조정하는 것입니다.

### **1.2 LSTM-AutoEncoder의 특징**

- **LSTM(Long Short-Term Memory)**은 순환 신경망(RNN)의 한 종류로, **시계열 데이터의 시간적 의존성**을 모델링하는 데 탁월합니다.
- **LSTM-AutoEncoder**는 AutoEncoder의 구조에 LSTM을 적용하여 **시계열 데이터의 패턴과 시간적 특징**을 학습합니다.
- 이를 통해 **정상적인 시계열 패턴을 학습**하고, **재구성 오류(reconstruction error)**를 기반으로 이상치를 탐지합니다.

---

## **2. 세션과 시퀀스의 개념**

### **2.1 세션(Session)**

- **세션**은 데이터 수집의 **연속적인 기간**을 의미합니다.
- 모터를 한 번 구동하여 수집한 데이터 집합이 하나의 세션이 됩니다.
- 세션 간에는 **시간적 연속성이 없습니다**. 즉, 세션이 끝나면 다음 세션은 별도의 시간대에서 시작됩니다.

### **2.2 시퀀스(Sequence)**

- **시퀀스**는 LSTM 모델이 한 번에 처리하는 **연속된 데이터 포인트의 집합**입니다.
- 시퀀스 길이(sequence length)는 모델 설계 시 결정하며, 예를 들어 50개의 타임스텝으로 구성될 수 있습니다.
- **중요한 점**: 시퀀스는 **세션 내에서만 생성**되며, **세션 경계를 넘어가지 않습니다**.

---

## **3. 데이터 구조와 흐름**

### **3.1 데이터 구조**

- **데이터 포인트**: 각 데이터 포인트는 시간에 따른 데이터 값으로 구성됩니다.
- **세션별 데이터**: 여러 세션이 있으며, 각 세션은 연속된 데이터 포인트의 집합입니다.

#### **예시**

- **Session 1**: 데이터 포인트 0 ~ 299
- **Session 2**: 데이터 포인트 0 ~ 399
- **Session 3**: 데이터 포인트 0 ~ 249
- ...

### **3.2 시퀀스 생성**

- 각 세션에서 **시퀀스를 생성**합니다.
- 시퀀스 길이가 50이라고 가정하면, 각 세션에서 가능한 시퀀스의 수는 `(세션 데이터 포인트 수) - 시퀀스 길이 + 1`입니다.

#### **Session 1 예시**

- **시퀀스 1**: 데이터 포인트 0 ~ 49
- **시퀀스 2**: 데이터 포인트 1 ~ 50
- ...
- **시퀀스 250**: 데이터 포인트 249 ~ 298

### **3.3 데이터 흐름**

1. **데이터 수집**: 각 세션에서 모터 구동 시 수집된 가속도 데이터를 확보합니다.

2. **데이터 전처리**:

   - **스케일링**: 모든 세션의 데이터를 합쳐서 `MinMaxScaler`나 `StandardScaler`를 사용하여 정규화합니다.
   - **결측치 처리**: 결측치가 있다면 제거하거나 보간합니다.

3. **시퀀스 생성**:

   - 각 세션별로 시퀀스를 생성합니다.
   - 시퀀스는 **세션 내의 데이터로만 구성**됩니다.
   - 생성된 시퀀스는 **3차원 배열**로 저장됩니다: `(총 시퀀스 수, 시퀀스 길이, 특징 수)`

4. **모델 학습**:

   - 생성된 모든 시퀀스를 사용하여 LSTM-AutoEncoder 모델을 학습합니다.
   - 모델은 입력 시퀀스를 압축하고 다시 복원하는 과정을 통해 **정상 패턴을 학습**합니다.

---

## **4. LSTM-AutoEncoder의 학습 원리와 데이터 흐름**

### **4.1 모델 구조**

- **인코더(Encoder)**:

  - LSTM 레이어를 사용하여 입력 시퀀스를 압축합니다.
  - 마지막 타임스텝의 출력은 시퀀스 전체의 압축된 표현입니다.

- **디코더(Decoder)**:

  - RepeatVector를 사용하여 압축된 표현을 시퀀스 길이만큼 복제합니다.
  - LSTM 레이어를 사용하여 시퀀스를 복원합니다.
  - TimeDistributed(Dense)를 통해 각 타임스텝의 출력을 얻습니다.

### **4.2 학습 과정**

1. **입력과 출력**:

   - 입력: 시퀀스 `(batch_size, sequence_length, features)`
   - 출력: 입력과 동일한 시퀀스

2. **손실 함수**:

   - **Mean Squared Error(MSE)**를 사용하여 입력 시퀀스와 복원된 시퀀스 간의 차이를 최소화합니다.

3. **학습 목표**:

   - 모델이 정상적인 시계열 패턴을 학습하여 입력 시퀀스를 정확하게 복원하도록 가중치를 조정합니다.

### **4.3 세션과 시퀀스의 관계**

- **세션별 학습 데이터**:

  - 각 세션에서 생성된 시퀀스들은 모델 학습 시 **독립적으로 처리되지 않고**, **전체 학습 데이터의 일부**로 사용됩니다.
  - 모델은 다양한 세션의 데이터를 통해 **다양한 정상 패턴**을 학습합니다.

- **시간적 의존성 유지**:

  - 시퀀스는 세션 내에서 시간적 순서를 유지하므로, LSTM이 시간적 패턴을 학습할 수 있습니다.
  - 세션 간의 불연속성은 시퀀스 생성 시 고려되므로, 모델 학습에 영향을 주지 않습니다.

---

## **5. 예시를 통한 데이터 흐름 설명**

### **5.1 데이터 준비**

- **Session 1 데이터** (`session1_data`):

  ```
  Index | acc_x | acc_y | acc_z
  --------------------------------
    0   | 0.1   | 0.2   | 0.15
    1   | 0.11  | 0.19  | 0.16
    ... (중략)
    299 | 0.12  | 0.18  | 0.14
  ```

- **Session 2 데이터** (`session2_data`):

  ```
  Index | acc_x | acc_y | acc_z
  --------------------------------
    0   | 0.09  | 0.21  | 0.13
    1   | 0.1   | 0.2   | 0.14
    ... (중략)
    399 | 0.11  | 0.19  | 0.15
  ```

### **5.2 시퀀스 생성 예시**

- **시퀀스 길이**: 50

- **Session 1의 시퀀스 생성**:

  - **시퀀스 1**:

    ```
    [ [0.1, 0.2, 0.15],
      [0.11, 0.19, 0.16],
      ... (중략)
      [데이터 포인트 49] ]
    ```

  - **시퀀스 2**:

    ```
    [ [0.11, 0.19, 0.16],
      [데이터 포인트 2],
      ... (중략)
      [데이터 포인트 50] ]
    ```

  - 총 생성 가능한 시퀀스 수: 300 - 50 + 1 = 251개

### **5.3 모델 학습 데이터 구성**

- **학습 데이터 `X_train`**:

  - Session 1의 시퀀스 251개
  - Session 2의 시퀀스 350개 (400 - 50 + 1)
  - ...
  - 모든 세션의 시퀀스를 합쳐서 총 `N`개의 시퀀스로 구성된 `(N, 50, 3)` 형태의 배열

### **5.4 모델 학습 단계**

1. **Forward Pass**:

   - 각 배치의 시퀀스가 인코더를 통과하여 압축된 표현으로 변환됩니다.
   - 디코더를 통해 시퀀스가 복원됩니다.

2. **손실 계산**:

   - 입력 시퀀스와 복원된 시퀀스 간의 MSE를 계산합니다.

3. **Backward Pass**:

   - 손실을 최소화하도록 모델의 가중치를 업데이트합니다.

4. **반복**:

   - 모든 배치를 처리할 때까지 위 과정을 반복합니다.
   - 에포크(epoch) 수만큼 학습을 진행합니다.

---

## **6. 이상치 탐지 과정**

### **6.1 재구성 오류 기반 이상치 탐지**

- **재구성 오류 계산**:

  - 새로운 데이터 시퀀스를 모델에 입력하고, 복원된 시퀀스를 얻습니다.
  - 입력 시퀀스와 복원된 시퀀스 간의 MSE를 계산하여 재구성 오류를 구합니다.

- **임계값 설정**:

  - 학습 데이터나 검증 데이터의 재구성 오류 분포를 기반으로 임계값을 설정합니다.
  - 일반적으로 재구성 오류가 임계값을 초과하면 이상치로 간주합니다.

### **6.2 실시간 데이터 처리 흐름**

1. **데이터 수신**:

   - 모터에서 실시간으로 가속도 데이터를 수집합니다.

2. **시퀀스 버퍼링**:

   - 일정량의 데이터 포인트를 버퍼에 저장하여 시퀀스를 구성합니다.
   - 버퍼가 시퀀스 길이에 도달하면 모델에 입력합니다.

3. **모델 예측 및 이상치 판단**:

   - 모델을 통해 시퀀스를 복원하고 재구성 오류를 계산합니다.
   - 재구성 오류가 임계값을 초과하는지 확인합니다.

4. **결과 처리**:

   - 이상치로 판단되면 알람 또는 로그를 생성합니다.
   - 버퍼를 업데이트하여 다음 시퀀스를 준비합니다.

---

## **7. 요약 및 결론**

- **LSTM-AutoEncoder의 학습 원리**는 입력 시퀀스를 압축하고 복원하는 과정을 통해 **정상적인 시계열 패턴을 학습**하는 것입니다.
- **세션과 시퀀스의 관계**:

  - 세션은 데이터 수집의 연속된 기간이며, 시퀀스는 세션 내에서 연속된 데이터 포인트로 구성됩니다.
  - 시퀀스는 세션 경계를 넘어가지 않도록 생성하여 LSTM이 시간적 패턴을 정확히 학습하도록 합니다.

- **데이터 구조와 흐름**은 세션별로 데이터를 관리하고, 각 세션에서 시퀀스를 생성하여 모델 학습에 사용합니다.
- **이상치 탐지**는 모델이 정상 패턴을 기반으로 입력 시퀀스를 복원할 때 재구성 오류를 계산하여 수행됩니다.

---

## **추가 참고 사항**

- **데이터 다양성**: 여러 세션의 데이터를 학습에 활용함으로써 모델이 다양한 정상 상태를 학습하게 되어 이상치 탐지의 신뢰성이 높아집니다.
- **모델 일반화**: 세션 간의 차이를 모델이 학습함으로써 새로운 세션에서도 정상과 이상을 효과적으로 구분할 수 있습니다.
- **실시간 적용**: 실시간 데이터 스트림에서도 버퍼를 사용하여 시퀀스를 구성하고 이상치를 탐지할 수 있습니다.

---

이러한 방식으로 LSTM-AutoEncoder의 학습 원리를 세션과 시퀀스의 개념과 연관 지어 이해하시면, 모델을 효과적으로 설계하고 적용하실 수 있을 것입니다. 추가로 궁금한 점이 있으시면 언제든지 말씀해 주세요!